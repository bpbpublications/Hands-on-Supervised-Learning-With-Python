{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [9],\n",
       "       [9],\n",
       "       [4],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "\n",
    "# inspect the data\n",
    "# print(x_train[0])\n",
    "print(X_train.shape)\n",
    "Y_train[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42) \n",
    "# set random seed for drop out etc... otherwise results are not reproducible / unreliable\n",
    "\n",
    "# define model name and initialize using the Sequential API (which allows us to easily build our network layer by layer)\n",
    "\n",
    "classier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0605 12:28:05.320252 4748301760 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- adding a 32 filters with dimensions of 3x3\n",
    "- padding to learn features at the edge\n",
    "- input shape is 32x32 pixel img, depth of 3 for RBG channels\n",
    "'''\n",
    "classier.add(Conv2D(32, kernel_size=(3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "'''\n",
    "- leakyReLU tend to perform better than RELU because there is a different slope (flatter slope, default = 0.3) for the negative values \n",
    "- standard ReLU propagates less information to the next layers because gradient is zero for all negative values\n",
    "- LeakyRelU is one solution (ELU also) that addresses the Dying RELU problem where a large gradient update causes the RELU neuron unit to never activate again\n",
    "'''\n",
    "classier.add(LeakyReLU(alpha=0.3)) \n",
    "classier.add(Conv2D(64, padding='same', kernel_size=(3, 3)))\n",
    "classier.add(LeakyReLU(alpha=0.3))\n",
    "classier.add(MaxPooling2D(pool_size=(2, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "- drop out is used to regularize neural networks\n",
    "- adding dropout after pooling layer tend to lead to better performance\n",
    "'''\n",
    "classier.add(Dropout(0.25)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the following block is similar in structure to previous blocks but with more filters\n",
    "'''\n",
    "classier.add(Conv2D(128, kernel_size=(3, 3)))\n",
    "classier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classier.add(Conv2D(128, kernel_size=(3, 3)))\n",
    "classier.add(LeakyReLU(alpha=0.3))\n",
    "classier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classier.add(Dropout(0.25))\n",
    "\n",
    "classier.add(Flatten()) # Flatten the feature map tensor to 1D array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "below is the classification Dense Neural Net\n",
    "typically a shallow DNN performs sufficiently and is faster to train \n",
    "'''\n",
    "classier.add(Dense(1024))\n",
    "classier.add(LeakyReLU(alpha=0.3))\n",
    "classier.add(Dropout(0.5))\n",
    "classier.add(Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "after the model architecture has been specified (roughly equivalent to specifying the computational graph)\n",
    "\n",
    "we can compile the model by providing the loss function and optimizer along with metrics you want to track\n",
    "\n",
    "it is appropriate to use categorical_crossentropy as a loss function because the softmax layer is size > 2 (if 2, use binary_crossentropy) and classes are mutually exclusive, if not use binary_crossentropy\n",
    "\n",
    "ADAM optimizer with a lower learning rate and low decay is a safe option especially because we are using early stop callback (see below), we are not concerned about training time for a relatively simple dataset especially when training on GPU\n",
    "'''\n",
    "classier.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.0001, decay=1e-6),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0605 12:29:05.702309 4748301760 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "50000/50000 [==============================] - 230s 5ms/step - loss: 1.9477 - accuracy: 0.2839 - val_loss: 1.7300 - val_accuracy: 0.3706\n",
      "Epoch 2/250\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.6390 - accuracy: 0.4048"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train the model:\n",
    "- the X_train / 255 is a way to normalize the data since the pixel values go from 0-255\n",
    "- not normalizing the data, especially for images, will lead to substantial degradation in model performance\n",
    "- we need to convert the labels (integer from 0-9) into a one-hot encoded vector of size 10 for the loss function\n",
    "- the early stop callback reduces overfitting, when the (by default) validation loss has not improved more than 0.01 in 3 epochs, we terminate training\n",
    "- another practical advantage of using early stop is it removes the need to tune number of epochs as a hyper parameter (set too low, model could have performed better, set too high overfit and waste compute / $)\n",
    "'''\n",
    "history = classier.fit(X_train / 255.0, to_categorical(Y_train),\n",
    "          batch_size=128,\n",
    "          shuffle=True,\n",
    "          epochs=250,\n",
    "          validation_data=(X_test / 255.0, to_categorical(Y_test)),\n",
    "          callbacks=[EarlyStopping(min_delta=0.01, patience=4)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy on the test set, it is important to normalize your training and test data the same way\n",
    "scores = classier.evaluate(X_test / 255.0, to_categorical(Y_test))\n",
    "\n",
    "print('Loss: %.3f' % scores[0])\n",
    "print('Accuracy: %.3f' % scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 108\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(X_test[sample_id])\n",
    "plt.axis('off')\n",
    "print(Y_test[sample_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "make a prediction using the model:\n",
    "- notice that the dimensions of the test data is the same as training data, it is 4D by default [index, height, width, channel]\n",
    "- we must reshape the sampled image to a shape the model expects by removing the outer dimension (remove the index dimension). This is the same reason why the label output from above is [8] instead of 8 by default.\n",
    "- we also need to normalize our data the same way we did during training\n",
    "'''\n",
    "\n",
    "softmax_output = classier.predict(X_test[108].reshape((1,) + X_test[108].shape)/255.)\n",
    "softmax_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(Y_test, Y_preds.argmax(axis=1))\n",
    "\n",
    "confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(confusion_matrix)\n",
    "plt.colorbar()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
